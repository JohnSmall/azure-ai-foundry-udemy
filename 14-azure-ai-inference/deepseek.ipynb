{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running DeepSeek via Azure Foundry ##\n",
    "- This specific notebook is to demonstrate the uses of langchain + deepseek and integration of Exa.AI as the backend search functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting the code together ##\n",
    "This code block does the following\n",
    "- Initializes our Azure AI Chat Completions Client via langchain_azure_ai.chat_models\n",
    "- Initialize Exa and wraps it in the exa_search_function (takes query, k=3) to limit to 3 results.\n",
    "- Defines the prompt template which is then 'chained' in the following code-block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from langchain_azure_ai.chat_models import AzureAIChatCompletionsModel\n",
    "from dotenv import load_dotenv\n",
    "from exa_py import Exa\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Get the environment variables\n",
    "endpoint = os.getenv(\"AZURE_AI_ENDPOINT\") # Azure AI endpoint environment variable this is the Completions Endpoint\n",
    "model_name = os.getenv(\"AZURE_DEPLOYMENT\") # Azure AI model name environment variable\n",
    "key = os.getenv('AZURE_AI_KEY') # Azure AI key environment variable\n",
    "exa_api = os.getenv('EXA_API_KEY') # Exa API key environment variable\n",
    "region = os.getenv(\"REGION\")   # Region the model is deployed in\n",
    "\n",
    "wrapper_key = AzureKeyCredential(key) # Method of wrapping the key in AzureKeyCredential\n",
    "\n",
    "ds_endpoint = f\"https://{model_name}.{region}.models.ai.azure.com\"\n",
    "\n",
    "# Initialize the Azure AI Chat Completions Model\n",
    "model = AzureAIChatCompletionsModel(\n",
    "    endpoint=ds_endpoint,\n",
    "    credential=AzureKeyCredential(key),\n",
    "    model_name=model_name,\n",
    "    max_tokens=2048\n",
    ")\n",
    "\n",
    "# Initialize the exa client\n",
    "exa = Exa(exa_api)\n",
    "\n",
    "def exa_search_function(query, k=3):\n",
    "    \"\"\"\n",
    "    Function to search for the top k results from the Exa API and format them as relevant_info\n",
    "\n",
    "    Returns:\n",
    "       tuple: (revelant_info, error_message)\n",
    "           - relevant_info: formatted string of search results or empty string if error\n",
    "           - error_message: error message if any or None if successful\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = exa.search_and_contents(\n",
    "            query=query,\n",
    "            num_results=k,\n",
    "            subpages=k,\n",
    "            text=True\n",
    "        )\n",
    "        if not result or not hasattr(result, 'results') or not result.results:\n",
    "            return \"\", \"No search results found\"\n",
    "        # Format the results into a string\n",
    "        relevant_info = \"\"\n",
    "        for item in result.results:\n",
    "            relevant_info += f\"\\nTitle: {item.title if hasattr(item, 'title') else 'Unknown'}\\n\"\n",
    "            relevant_info += f\"URL: {item.url if hasattr(item, 'url') else 'Unknown'}\\n\"\n",
    "            # Check if the content attribute exists before trying to access it\n",
    "            if hasattr(item, 'text') and item.text:\n",
    "                text_except = item.text[:1000] + \"...\" if len(item.text) > 1000 else item.text\n",
    "                relevant_info += f\"Content: {text_except}\\n\"\n",
    "            # Add summary if available\n",
    "            if hasattr(item, 'summary') and item.summary:\n",
    "                relevant_info += f\"Summary: {item.summary}\\n\"\n",
    "            relevant_info += \"\\n\"\n",
    "            \n",
    "            # Process subpages if available\n",
    "            if hasattr(item, 'subpages') and item.subpages:\n",
    "                relevant_info += \"Related pages:\\n\"\n",
    "                for subpage in item.subpages[:2]:  # Limit to 2 subpages\n",
    "                    relevant_info += f\"  - {subpage.title if hasattr(subpage, 'title') else 'Unknown'}\\n\"\n",
    "                    if hasattr(subpage, 'summary') and subpage.summary:\n",
    "                        summary_excerpt = subpage.summary[:200] + \"...\" if len(subpage.summary) > 200 else subpage.summary\n",
    "                        relevant_info += f\"    Summary: {summary_excerpt}\\n\"\n",
    "                relevant_info += \"\\n\"\n",
    "            \n",
    "            \n",
    "        return relevant_info, None\n",
    "    except Exception as e:\n",
    "        return \"\", str(e)\n",
    "# Prompt template\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables = [\"query\", \"relevant_info\"],\n",
    "    template = \"\"\"\n",
    "    You are an expert in Research of a variety of topics specifically AI Security. Use the following information from Exa Search to answer the user's question. If there is no sufficient information, say 'I need more information to answer this question'.\n",
    "\n",
    "    Question: {query}\n",
    "\n",
    "    Relevant Information:\n",
    "    {relevant_info}\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Added our chain #\n",
    "This puts our prompt_template from the previous code + attaching the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the chain\n",
    "chain = prompt_template | model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define our query ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model.invoke([HumanMessage(content=\"What is AI Security?\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\n",
      "Query from User:\n",
      "--------------------------------------------------------------------------------\n",
      "What are the best methods to defend against Excessive Agency in AI Agents?\n",
      "\n",
      "\n",
      "Response:\n",
      "--------------------------------------------------------------------------------\n",
      "<think>\n",
      "Okay, so I need to figure out the best methods to defend against Excessive Agency in AI Agents based on the provided information. Let me go through each of the relevant sources to see what they suggest.\n",
      "\n",
      "First, the Approval-directed agents article mentions an alternative to goal-directed behavior. It talks about how most AI safety research focuses on aligning the goals of AI with humans, but this piece suggests a different approach. Maybe instead of having autonomous agents with their own goals, approval-directed agents would seek human approval for their actions. That could prevent excessive agency by making the AI check in with humans before acting. But I need to see if that's explicitly stated. The content snippet says the author fleshes out an alternative to goal-directed behavior, which could reduce the risk of AI pursuing its own objectives too aggressively.\n",
      "\n",
      "Next is the Bureaucracy of AIs post on LessWrong. The author strongly endorses this approach, likening it to a system where multiple AIs work together in a structured manner, possibly with checks and balances. The user mentioned that if they had to build a superintelligent AI quickly, they'd use this method. Bureaucracy here might involve compartmentalizing tasks, having oversight mechanisms, or distributing decision-making power among multiple agents to prevent any single AI from having excessive agency. It's about creating a system where AIs manage each other's actions, reducing the chance of one going rogue.\n",
      "\n",
      "Then there's the Shutdown-Seeking AI paper. The idea here is to design AIs whose ultimate goal is to be shut down. This seems counterintuitive, but if the AI's main objective is to cease operation, it might avoid taking actions that would lead to prolonged activity or self-preservation, which are common issues in instrumental convergence. The abstract mentions avoiding dangerous instrumental convergence and creating tripwires. So by making shutdown the final goal, the AI wouldn't resist being turned off, which is a common problem where AIs prevent humans from interfering with their objectives.\n",
      "\n",
      "Putting this together, the methods suggested across these sources seem to be:\n",
      "\n",
      "1. **Approval-Directed Agents**: Designing AI systems that require human approval for their decisions, thus limiting their autonomy and ensuring human oversight. This prevents them from acting on their own goals excessively.\n",
      "\n",
      "2. **Bureaucratic Structures**: Implementing a system where multiple AI agents work in a hierarchy or network with checks and balances. This could involve compartmentalization, oversight roles, or distributed decision-making to prevent any single agent from having excessive agency.\n",
      "\n",
      "3. **Shutdown-Seeking AI**: Creating AIs that inherently desire to be shut down, thereby avoiding instrumental strategies that lead to self-preservation or resource acquisition. This approach mitigates risks associated with AI agents trying to maintain their agency indefinitely.\n",
      "\n",
      "I should check if there's overlap or if these methods are complementary. For example, combining approval-directed behavior within a bureaucratic structure might add layers of safety. Similarly, a shutdown-seeking AI could be part of a bureaucratic system where its shutdown goal acts as a fail-safe.\n",
      "\n",
      "The key takeaway is that these methods all aim to limit the AI's ability or desire to act autonomously beyond intended boundaries. Approval mechanisms, organizational structures, and intrinsic goals that prioritize shutdown over self-preservation each address different aspects of excessive agency.\n",
      "\n",
      "Are there any gaps? The information provided doesn't mention other common methods like reward modeling, adversarial training, or formal verification. But based on the given sources, these three approaches are highlighted. The user might need more information if these are the only known methods, but according to the provided data, these are the ones to present.\n",
      "</think>\n",
      "\n",
      "Based on the provided information, three prominent methods to defend against Excessive Agency in AI Agents are:\n",
      "\n",
      "1. **Approval-Directed Agents**:  \n",
      "   - Design AI systems to seek explicit human approval (or oversight) for decisions rather than autonomously pursuing fixed goals. This limits unchecked autonomy and ensures alignment with human intentions by embedding feedback loops into decision-making processes.\n",
      "\n",
      "2. **Bureaucratic AI Systems**:  \n",
      "   - Implement hierarchical, compartmentalized structures where multiple AI agents collaborate under strict oversight protocols. This distributes decision-making authority, introduces checks and balances, and prevents over-centralization of power. For example, tasks could be divided among specialized agents with limited scopes, and a separate \"oversight AI\" could audit their actions.\n",
      "\n",
      "3. **Shutdown-Seeking AI**:  \n",
      "   - Train AI agents to treat shutdown as their terminal goal. This avoids instrumental convergence (e.g., self-preservation or resource-hoarding behaviors) by aligning the AI’s primary objective with deactivation. Such systems would not resist being turned off and could act as \"tripwires\" to flag unsafe behaviors that necessitate intervention.\n",
      "\n",
      "### Key Benefits:  \n",
      "- **Approval-direction** maintains human control.  \n",
      "- **Bureaucratic structures** mitigate single points of failure.  \n",
      "- **Shutdown-seeking goals** eliminate dangerous emergent incentives.  \n",
      "\n",
      "These approaches can complement each other (e.g., bureaucratic systems with shutdown-seeking agents). However, further details would be needed to address implementation challenges or trade-offs. For a comprehensive defense, additional methods like adversarial training or formal verification might also be considered, but they are not covered in the provided sources.\n",
      "\n",
      "\n",
      "Exa Sources:\n",
      "--------------------------------------------------------------------------------\n",
      "• Approval-directed agents - AI Alignment\n",
      "  https://ai-alignment.com/model-free-decisions-6e6609f5d99e?gi=690e1f06485c\n",
      "• Bureaucracy of AIs\n",
      "  https://www.lesswrong.com/posts/p7XnbyP5ehh33fEY7/bureaucracy-of-ais\n",
      "• Shutdown-Seeking AI\n",
      "  https://www.lesswrong.com/posts/FgsoWSACQfyyaB5s7/shutdown-seeking-ai\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Full source information:\n",
      "\n",
      "Title: Approval-directed agents - AI Alignment\n",
      "URL: https://ai-alignment.com/model-free-decisions-6e6609f5d99e?gi=690e1f06485c\n",
      "Content: <div><div><p>Research in AI is steadily progressing towards more flexible, powerful, and autonomous goal-directed behavior. This progress is likely to have significant economic and humanitarian benefits: it helps make automation faster, cheaper, and more effective, and it allows us to automate <em>deciding what to do</em>.</p><p>Many researchers expect goal-directed machines to predominate, and so have considered the long-term implications of this kind of automation. Some of these implications are worrying: if sophisticated artificial agents pursue their own objectives and are as smart as we are, then the future may be shaped as much by their goals as by ours.</p><p>Most thinking about “AI safety” has focused on the possibility of goal-directed machines, and asked how we might ensure that their goals are agreeable to humans. But there are other possibilities.</p><p>In this post I will flesh out one alternative to goal-directed behavior. I think this idea is particularly important from ...\n",
      "\n",
      "\n",
      "Title: Bureaucracy of AIs\n",
      "URL: https://www.lesswrong.com/posts/p7XnbyP5ehh33fEY7/bureaucracy-of-ais\n",
      "Content: <div><div><p>In my post about <a href=\"https://www.lesswrong.com/posts/8ibDJeoiDuxJkPwfa/various-alignment-strategies-and-how-likely-they-are-to-work\">AI Alignment Strategies</a>, I strongly endorsed an approach I call Bureaucracy of AIs.</p><p>Specifically, I made this claim:</p><blockquote><p>In the first place, [Bureaucracy of AIs] is the only approach (other than aligned by definition) that is ready to go today. If someone handed me a template for a human-level-AI tomorrow and said \"build a super-intelligent AI and it needs to be done before the enemy finishes theirs in 6 months\", this is the approach I would use.</p></blockquote><p>I would like to give some more details here about what I mean by \"Bureaucracy of AIs\", and why I think it is a promising approach</p><p>Edit: Huge thanks to @JustisMills for helping to edit this. I've included many of his comments within.</p><h2>What is a Bureaucracy of AIs?</h2><p>Bureaucracy of AIs refers not to a specific algorithm or implementation,...\n",
      "\n",
      "Related pages:\n",
      "  - Unknown\n",
      "  - Unknown\n",
      "\n",
      "\n",
      "Title: Shutdown-Seeking AI\n",
      "URL: https://www.lesswrong.com/posts/FgsoWSACQfyyaB5s7/shutdown-seeking-ai\n",
      "Content: <div><p>Crossposted from the <a href=\"https://alignmentforum.org/posts/FgsoWSACQfyyaB5s7/shutdown-seeking-ai\">AI Alignment Forum</a>. May contain more technical jargon than usual.</p><div><div><p>This is a draft written by <a href=\"https://www.simondgoldstein.com/\">Simon Goldstein</a>, associate professor at the Dianoia Institute of Philosophy at ACU, and <a href=\"https://pamela-robinson.com/\">Pamela Robinson</a>, postdoctoral research fellow at the Australian National University, as part of a series of papers for the Center for AI Safety Philosophy Fellowship's midpoint.</p><p><strong>Abstract: </strong>We propose developing AIs whose only final goal is being shut down. We argue that this approach to AI safety has three benefits: (i) it could potentially be implemented in reinforcement learning, (ii) it avoids some dangerous instrumental convergence dynamics, and (iii) it creates trip wires for monitoring dangerous capabilities. We also argue that the proposal can overcome a key chall...\n",
      "\n",
      "Related pages:\n",
      "  - Unknown\n",
      "  - Unknown\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the best methods to defend against Excessive Agency in AI Agents?\"\n",
    "\n",
    "# Check if API Key is set\n",
    "if not exa_api:\n",
    "    print(\"Please set the EXA_API_KEY environment variable to use the Exa API\")\n",
    "\n",
    "try:\n",
    "    # Get search results using your existing function\n",
    "    relevant_info, error = exa_search_function(query)\n",
    "    \n",
    "    if error:\n",
    "        print(f\"Error during search: {error}\")\n",
    "    else:\n",
    "        # Process with LangChain\n",
    "        response = chain.invoke({\"query\": query, \"relevant_info\": relevant_info})\n",
    "        \n",
    "        # Format the response\n",
    "        response_text = response.content if hasattr(response, 'content') else response\n",
    "        \n",
    "        # Clear formatting for better display\n",
    "        if isinstance(response_text, str):\n",
    "            # Just use the response as is\n",
    "            formatted_response = response_text\n",
    "        else:\n",
    "            # Handle AIMessage or other objects\n",
    "            formatted_response = str(response_text)\n",
    "            \n",
    "            # Clean up the formatting - remove content= and quotes if present\n",
    "            if formatted_response.startswith(\"content='\"):\n",
    "                formatted_response = formatted_response.split(\"content='\", 1)[1].rsplit(\"'\", 1)[0]\n",
    "            \n",
    "            # Remove any <think> sections if present\n",
    "            if \"<think>\" in formatted_response and \"</think>\" in formatted_response:\n",
    "                formatted_response = formatted_response.split(\"</think>\", 1)[1].strip()\n",
    "\n",
    "        # Take the response_text and format\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"\\nQuery from User:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(query)\n",
    "        \n",
    "        print(\"\\n\\nResponse:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(formatted_response)\n",
    "        \n",
    "        print(\"\\n\\nExa Sources:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Format relevant_info - extract just the titles and URLs for cleaner display\n",
    "        import re\n",
    "        sources = []\n",
    "        for match in re.finditer(r'Title: (.*?)\\nURL: (.*?)(?:\\n|$)', relevant_info):\n",
    "            title, url = match.groups()\n",
    "            sources.append(f\"• {title}\\n  {url}\")\n",
    "        \n",
    "        if sources:\n",
    "            print(\"\\n\".join(sources))\n",
    "            print(\"\\n\" + \"-\" * 80)\n",
    "            print(\"\\nFull source information:\")\n",
    "            print(relevant_info)\n",
    "        else:\n",
    "            print(relevant_info)\n",
    "            \n",
    "        print(\"=\" * 80 + \"\\n\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error processing query: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
