{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LlamaIndex + Azure AI Search ##\n",
    "This notebook is intended to demonstrate the integration between Azure AI Search with LlamaIndex as adapted by https://docs.llamaindex.ai/en/stable/examples/vector_stores/AzureAISearchIndexDemo/\n",
    "\n",
    "Pre-requisites\n",
    "- An Azure OpenAI Embedding Model Deployed for this demo I'm using text-ada-002\n",
    "- Chat Completions Model deployed this is gpt-4o,gpt-4o-mini, gpt-3.5 etc\n",
    "- Azure AI Search Deployed with API Keys, Endpoints, API Version this will have a helper script to deploy these for you if you don't have them deployed.\n",
    "\n",
    "We will need imports assuming none of these are already installed the following block puts llama-index and other packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index\n",
    "!pip install wget\n",
    "%pip install llama-index-vector-stores-azureaisearch\n",
    "%pip install azure-search-documents==11.5.1\n",
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from IPython.display import Markdown, display\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    VectorStoreIndex,\n",
    ")\n",
    "from llama_index.core.settings import Settings\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "from llama_index.vector_stores.azureaisearch import AzureAISearchVectorStore\n",
    "from llama_index.vector_stores.azureaisearch import (\n",
    "    IndexManagement,\n",
    "    MetadataIndexFieldType,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup our Azure OpenAI this will have our Chat Completions Client ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the environment variables\n",
    "load_dotenv()\n",
    "\n",
    "azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "azure_openai_api_key = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "chat_completions = os.getenv(\"CHAT_COMPLETIONS_MODEL\")\n",
    "embed_model_name = os.getenv(\"EMBEDDING_DEPLOYMENT_NAME\")\n",
    "api_version = \"2024-02-15-preview\"\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    deployment_name=chat_completions,\n",
    "    api_key=azure_openai_api_key,\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    api_version=api_version,\n",
    ")\n",
    "\n",
    "# You need to deploy your own embedding model as well as your own chat completion model\n",
    "embed_model = AzureOpenAIEmbedding(\n",
    "    model=embed_model_name,\n",
    "    deployment_name=embed_model_name,\n",
    "    api_key=azure_openai_api_key,\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    api_version=api_version,\n",
    ")\n",
    "\n",
    "\n",
    "embed_model\n",
    "llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Azure AI Search ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the environment variables\n",
    "load_dotenv()\n",
    "\n",
    "search_service_api_key = os.getenv(\"SEARCH_INDEX_API_KEY\")\n",
    "search_service_endpoint = os.getenv(\"SEARCH_INDEX_ENDPOINT\")\n",
    "search_service_api_version = os.getenv(\"SEARCH_SERVICE_API_VERSION\")\n",
    "credential = AzureKeyCredential(search_service_api_key)\n",
    "\n",
    "\n",
    "# Index name to use\n",
    "index_name = \"llamaindex-vector-demo-1\"\n",
    "\n",
    "# Use index client to demonstrate creating an index\n",
    "index_client = SearchIndexClient(\n",
    "    endpoint=search_service_endpoint,\n",
    "    credential=credential,\n",
    ")\n",
    "\n",
    "# Use search client to demonstration using existing index\n",
    "search_client = SearchClient(\n",
    "    endpoint=search_service_endpoint,\n",
    "    index_name=index_name,\n",
    "    credential=credential,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a index (if it doesn't exist) ##\n",
    "The above code creates 'llamaindex-vector-demo' if it doesn't exist. This specific index has the following fields\n",
    "id - Edm.String\n",
    "chunk - Edm.String\n",
    "embedding - Collection(Edm.Single)\n",
    "metadata - Edm.String\n",
    "doc_id - Edm.String\n",
    "author - Edm.String\n",
    "theme - Edm.String\n",
    "director - Edm.String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_fields = {\n",
    "    \"author\": \"author\",\n",
    "    \"theme\": (\"topic\", MetadataIndexFieldType.STRING),\n",
    "    \"director\": \"director\",\n",
    "}\n",
    "\n",
    "vector_store = AzureAISearchVectorStore(\n",
    "    search_or_index_client=index_client,\n",
    "    filterable_metadata_field_keys=metadata_fields,\n",
    "    index_name=index_name,\n",
    "    index_management=IndexManagement.CREATE_IF_NOT_EXISTS,\n",
    "    id_field_key=\"id\",\n",
    "    chunk_field_key=\"chunk\",\n",
    "    embedding_field_key=\"embedding\",\n",
    "    embedding_dimensionality=1536,\n",
    "    metadata_string_field_key=\"metadata\",\n",
    "    doc_id_field_key=\"doc_id\",\n",
    "    language_analyzer=\"en.lucene\",\n",
    "    vector_algorithm_type=\"exhaustiveKnn\",\n",
    "    # compression_type=\"binary\" # Option to use \"scalar\" or \"binary\". NOTE: compression is only supported for HNSW\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents\n",
    "documents = SimpleDirectoryReader(\"./data/\").load_data()\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the data ##\n",
    "This uses the concept of a query engine this is accessed by index and shown with a similarity_top_k parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the data\n",
    "query_engine = index.as_query_engine(similarity_top_k=3)\n",
    "response = query_engine.query(\"What is Prompt Injection?\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\n",
    "    \"What is Excessive Agency?\",\n",
    ")\n",
    "display(Markdown(f\"{response}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing a vector search ##\n",
    "Four query modes are supported: Default (Vector Search), Sparse, Hyrbid, and Semantic_Hybrid for the default retriever notice we are passing DEFAULT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.vector_stores.types import VectorStoreQueryMode\n",
    "\n",
    "\n",
    "default_retriever = index.as_retriever(\n",
    "    vector_store_query_mode=VectorStoreQueryMode.DEFAULT\n",
    ")\n",
    "response = default_retriever.retrieve(\"What is Model Theft about?\")\n",
    "\n",
    "# Loop through each NodeWithScore in the response\n",
    "for node_with_score in response:\n",
    "    node = node_with_score.node  # The TextNode object\n",
    "    score = node_with_score.score  # The similarity score\n",
    "    chunk_id = node.id_  # The chunk ID\n",
    "\n",
    "    # Extract the relevant metadata from the node\n",
    "    file_name = node.metadata.get(\"file_name\", \"Unknown\")\n",
    "    file_path = node.metadata.get(\"file_path\", \"Unknown\")\n",
    "\n",
    "    # Extract the text content from the node\n",
    "    text_content = node.text if node.text else \"No content available\"\n",
    "\n",
    "    # Print the results in a user-friendly format\n",
    "    print(f\"Score: {score}\")\n",
    "    print(f\"File Name: {file_name}\")\n",
    "    print(f\"Id: {chunk_id}\")\n",
    "    print(\"\\nExtracted Content:\")\n",
    "    print(text_content)\n",
    "    print(\"\\n\" + \"=\" * 40 + \" End of Result \" + \"=\" * 40 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now trying this out with Hybrid Search ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.vector_stores.types import VectorStoreQueryMode\n",
    "\n",
    "hybrid_retriever = index.as_retriever(\n",
    "    vector_store_query_mode=VectorStoreQueryMode.HYBRID\n",
    ")\n",
    "hybrid_retriever.retrieve(\"What is Prompt Injection?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform a Hybrid Search with Semantic Reranking ##\n",
    "- Semantic search is a concept covered more in detail by docs located https://learn.microsoft.com/azure/search/semantic-search-overview\n",
    "- This is available for SKU's that are beyond the free tier therefore it didn't work on my end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "import json\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.core.exceptions import AzureError\n",
    "\n",
    "# Load the environment variables\n",
    "load_dotenv()\n",
    "\n",
    "search_service_api_key = os.getenv(\"SEARCH_INDEX_API_KEY\")\n",
    "search_service_endpoint = os.getenv(\"SEARCH_INDEX_ENDPOINT\")\n",
    "search_service_api_version = os.getenv(\"SEARCH_SERVICE_API_VERSION\")\n",
    "azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "azure_openai_api_key = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "api_version = os.getenv(\"API_VERSION\")\n",
    "deployment_name = os.getenv(\"DEPLOYMENT_NAME\")\n",
    "embed_model_name = os.getenv(\"EMBEDDING_DEPLOYMENT_NAME\")\n",
    "credential = AzureKeyCredential(search_service_api_key)\n",
    "\n",
    "# Index name to use\n",
    "index_name = \"llamaindex-vector-demo-1\"\n",
    "\n",
    "try:\n",
    "    # Native AzureOpenAI initiated\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=azure_openai_endpoint,\n",
    "        api_key=azure_openai_api_key,\n",
    "        api_version=api_version,\n",
    "    )\n",
    "\n",
    "    completion = client.chat.completions.create(  \n",
    "              model=deployment_name,  \n",
    "              messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an AI assistant that helps people find information.\"\n",
    "        }\n",
    "    ],    \n",
    "              max_tokens=800,  \n",
    "              temperature=0.7,  \n",
    "              top_p=0.95,  \n",
    "              frequency_penalty=0,  \n",
    "              presence_penalty=0,  \n",
    "              stop=None,  \n",
    "              extra_body={  \n",
    "                  \"data_sources\": [  \n",
    "                      {  \n",
    "                          \"type\": \"azure_search\",  \n",
    "                          \"parameters\": {  \n",
    "                              \"endpoint\": search_service_endpoint,  \n",
    "                              \"index_name\": index_name,  \n",
    "                              \"authentication\": {  \n",
    "                                  \"type\": \"api_key\",\n",
    "                                  \"api_key\": search_service_api_key \n",
    "                              }  \n",
    "                          }  \n",
    "                      }  \n",
    "                  ]  \n",
    "              }  \n",
    "          )  \n",
    "            \n",
    "    print(completion.model_dump_json(indent=2))\n",
    "\n",
    "except AzureError as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "red",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
