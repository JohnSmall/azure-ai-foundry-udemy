1. **Prompt Injection** - Manipulating input prompts to alter LLM behavior unexpectedly.
2. **Insecure Output Handling** - Vulnerability occurs when an LLM output is accepted without scrutiny, exposing backend systems.
3. **Training Data Poisoning** - Introducing malicious data to compromise model integrity.
4. **Model Denial of Service** - Attacks cause resource-heavy operations on LLMs, leading to service degradation or high costs.
5. **Supply Chain Vulnerabilities** - Risks within LLM supply chains and dependencies.
6. **Sensitive Information Disclosure** - LLMs may inadvertently reveal confidential data in their responses, leading to unauthorized data access, privacy violations, and security breaches.
7. **Insecure Plugin Design** - LLM plugins can have insecure inputs and insufficient access control.
8. **Excessive Agency** - LLM-based systems may undertake actions leading to unintended consequences.
9. **Overreliance** - System or people overly depending on LLMs without oversight may face misinformation, miscommunication, legal issues, and security vulnerabilities due to incorrect or inappropriate content generated by LLMs.
10. **Model Theft** - This involves unauthorized access, copying, or exfiltration of proprietary LLM models.
