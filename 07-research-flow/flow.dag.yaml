$schema: https://azuremlschemas.azureedge.net/promptflow/latest/Flow.schema.json
environment:
  python_requirements_txt: requirements.txt
inputs:
  chat_history:
    type: list
    default: []
  question:
    type: string
    default: Large Language Model Security Risks
    is_chat_input: true
outputs:
  answer:
    type: string
    reference: ${augmented_chat.output}
    is_chat_output: true
nodes:
- name: extract_query_from_question
  type: llm
  source:
    type: code
    path: extract_query_from_question.jinja2
  inputs:
    deployment_name: <deployment_name> # replace with the actual deployment name of azure openai
    model: gpt-4 #<model used for deployment>
    temperature: 0.7
    top_p: 1
    max_tokens: 1000
    presence_penalty: 0
    frequency_penalty: 0
    question: ${inputs.question}
    chat_history: ${inputs.chat_history}
  connection: open_ai_connection
  api: chat
- name: get_arxiv
  type: python
  source:
    type: code
    path: get_arxiv.py
  inputs:
    entity: ${inputs.question}
    count: 3
- name: process_search
  type: python
  source:
    type: code
    path: process_search.py
  inputs:
    results: ${get_arxiv.output}
- name: augmented_chat
  type: llm
  source:
    type: code
    path: augmented_chat.jinja2
  inputs:
    deployment_name: <deployment-name> # replace with your deployment name
    model: gpt-4 # <if using a separate model/serverless this will change>
    temperature: 0.8
    max_tokens: 1000
    response_format:
      type: text
    contexts: ${process_search.output}
    question: ${inputs.question}
    chat_history: ${inputs.chat_history}
  connection: open_ai_connection
  api: chat
